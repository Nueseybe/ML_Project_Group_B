{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_result = pd.concat([lr_result, dt_result, rf_result, et_result, ad_result, xgb_result, lgbm_result])\n",
    "modeling_result.sort_values(by='NDCG Score', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Model\t                NDCG Score\n",
    "0\tLGBMClassifier\t        0.8496 +/- 0.0006\n",
    "0\tXGBClassifier\t        0.8482 +/- 0.0004\n",
    "0\tRandomForestClassifier\t0.8451 +/- 0.0006\n",
    "0\tAdaBoostClassifier\t    0.8429 +/- 0.0019\n",
    "0\tExtraTreesClassifier\t0.839 +/- 0.0008\n",
    "0\tLogisticRegression\t    0.8378 +/- 0.001\n",
    "0\tDecisionTreeClassifier\t0.7242 +/- 0.0023\n",
    "\n",
    "The <b>Light GBM Classifier</b> model was chosen for hyperparameter tuning, since it's fast to train and tune, whilst being also the one with the best result without any tuning. In addition to that, it's much better for deployment, as it's much lighter than a XGBoost or Random Forest for instance, especially given the fact that we're using a free deployment cloud.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/brunodifranco/project-airbnb-classification/blob/main/airbnb.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important task in ML modeling is Hyperparameter Tuning, on which the goal is to find the <b>best possible combination of model hyperparameters</b>. This task will be performed fitting the model to the training data, and evaluating it in the test data, which was originally split in section 5.2. But firstly the test dataset has to go through all transformations the training dataset went through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_le = LabelEncoder()\n",
    "df_test['country_destination'] = test_le.fit_transform(df_test['country_destination'])\n",
    "\n",
    "X_test = df_test.drop('country_destination', axis=1).copy()\n",
    "y_test = df_test['country_destination'].copy()\n",
    "\n",
    "# One Hot Encoding \n",
    "X_test = pd.get_dummies(X_test, prefix=['gender'], columns=['gender'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['signup_method'], columns=['signup_method'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['signup_app'], columns=['signup_app'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['affiliate_channel'], columns=['affiliate_channel'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['first_affiliate_tracked'], columns=['first_affiliate_tracked'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['first_device_type'], columns=['first_device_type'])\n",
    "\n",
    "# FrequencyEncoder\n",
    "fe_language = (X_test.groupby('language').size()) / len(X_test)\n",
    "X_test['language'] = X_test['language'].apply(lambda x : fe_language[x])\n",
    "\n",
    "fe_action_type_most_common = (X_test.groupby('action_type_most_common').size()) / len(X_test)\n",
    "X_test['action_type_most_common'] = X_test['action_type_most_common'].apply(lambda x : fe_action_type_most_common[x])\n",
    "\n",
    "# RobustScaler\n",
    "rs_age = RobustScaler()\n",
    "rs_signup_flow = RobustScaler()\n",
    "rs_secs_elapsed_median = RobustScaler()\n",
    "X_test['age'] = rs_age.fit_transform(X_test[['age']].values)\n",
    "X_test['signup_flow'] = rs_signup_flow.fit_transform(X_test[['signup_flow']].values)\n",
    "X_test['secs_elapsed_median'] = rs_secs_elapsed_median.fit_transform(X_test[['secs_elapsed_median']].values)\n",
    "\n",
    "# MinMaxScaler\n",
    "mm_secs_elapsed_max = MinMaxScaler()\n",
    "mm_secs_elapsed_mean = MinMaxScaler()\n",
    "mm_secs_elapsed_sum = MinMaxScaler()\n",
    "mm_secs_elapsed_std = MinMaxScaler()\n",
    "mm_amount_of_sessions = MinMaxScaler()\n",
    "X_test['secs_elapsed_max'] = mm_secs_elapsed_max.fit_transform(X_test[['secs_elapsed_max']].values)\n",
    "X_test['secs_elapsed_mean'] = mm_secs_elapsed_mean.fit_transform(X_test[['secs_elapsed_mean']].values)\n",
    "X_test['secs_elapsed_sum'] = mm_secs_elapsed_sum.fit_transform(X_test[['secs_elapsed_sum']].values)\n",
    "X_test['secs_elapsed_std'] = mm_secs_elapsed_std.fit_transform(X_test[['secs_elapsed_std']].values)\n",
    "X_test['amount_of_sessions'] = mm_amount_of_sessions.fit_transform(X_test[['amount_of_sessions']].values)\n",
    "\n",
    "# StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_test['action_type_unique'] = ss.fit_transform(X_test[['action_type_unique']].values)\n",
    "\n",
    "# Transformation\n",
    "cols = {'day_of_week_first_active': 7,   \n",
    "        'month_account_created' : 12, \n",
    "        'day_first_active': 30,\n",
    "        'week_of_year_account_created': 52}\n",
    "\n",
    "for period, cycle in cols.items():\n",
    "    nature_encode(X_test, period, cycle)\n",
    "\n",
    "# final X_test\n",
    "X_test = X_test[cols_selected_rf]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the tested hyperparameters, Bayesian Optimization with Optuna provided the following as the best ones:\n",
    "\n",
    "| Hyperparameter | Definition | Best Value |\n",
    "|:---:|---|:---:|\n",
    "| n_estimators | Number of boosting iterations | 300 |\n",
    "| learning_rate | Shrinkage rate | 0.02 |\n",
    "| num_leaves | max number of leaves in one tree | 45 |\n",
    "| max_depth | Limit the max depth for tree model | 8 |\n",
    "| min_child_samples | Minimal number of data in one leaf | 55 |\n",
    "| min_child_weight | Minimal sum hessian in one leaf | 0.04 |\n",
    "| subsample | Used to randomly select part of data without resampling| 0.8 |\n",
    "| colsample_bytree| Used to randomly select a subset of features on each iteration| 0.85 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned Parameters\n",
    "best_param =  {'n_estimators': 300,\n",
    "               'learning_rate': 0.02,\n",
    "               'num_leaves': 45,\n",
    "               'max_depth': 8,\n",
    "               'min_child_samples': 55,\n",
    "               'min_child_weight': 0.04,\n",
    "               'subsample': 0.8,\n",
    "               'colsample_bytree': 0.85}\n",
    "\n",
    "# Final Model\n",
    "model_lgbm_final = LGBMClassifier(**best_param)\n",
    "\n",
    "y_pred_eval, df_model_eval = model_eval(model_lgbm_final, X_train_ml, y_train_ml, X_test, y_test)\n",
    "# pickle.dump(model_lgbm_final, open('model/lgbm_airbnb.pkl', 'wb'))  # Saving for deployment\n",
    "\n",
    "df_model_eval # final model score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgbm i secmis ve hyper parametreleri bulup tune etmis \n",
    "\n",
    "https://github.com/brunodifranco/project-airbnb-classification/blob/main/airbnb.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features must be str or int for catboost model\n",
    "\n",
    "df[cat_features] = df[cat_features].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(iterations=2000, \n",
    "                          learning_rate=0.05,\n",
    "                          depth=10,\n",
    "                          l2_leaf_reg=15,\n",
    "                          loss_function='Huber:delta=1.6',\n",
    "                          # save_snapshot='\n",
    "                         )\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=cat_features,\n",
    "    verbose=250,\n",
    "    eval_set=(X_validation, y_validation),\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "Look at feature importance and chose only the features that help the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portances = model.get_feature_importance()\n",
    "\n",
    "sorted_importances = sorted(importances, reverse=True)\n",
    "\n",
    "threshold = sorted_importances[len(sorted_importances) // 2]\n",
    "\n",
    "selected_features = [f for i, f in enumerate(X_train.columns) if importances[i] >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new list of categorical features in the new df\n",
    "all_columns = list(X_train[selected_features].columns)\n",
    "num_columns = ['accommodates', 'bedrooms', 'beds', 'minimum_nights', 'maximum_nights', 'bathrooms', 'price']\n",
    "\n",
    "cat_features_50 = []\n",
    "for column in all_columns:\n",
    "    if column not in num_columns:\n",
    "        cat_features_50.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with top 50 features\n",
    "model_2 = CatBoostRegressor(iterations=2000, \n",
    "                          learning_rate=0.08,\n",
    "                          depth=10,\n",
    "                          l2_leaf_reg=15,\n",
    "                          loss_function='Huber:delta=1.1',\n",
    "                          #save_snapshot=True\n",
    "                         )\n",
    "\n",
    "model_2.fit(\n",
    "    X_train[selected_features], y_train,\n",
    "    cat_features=cat_features_50,\n",
    "    verbose=250,\n",
    "    eval_set=(X_validation[selected_features], y_validation),\n",
    "    plot=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning\n",
    "model_3 = CatBoostRegressor(loss_function='Huber:delta=1.1',\n",
    "                            cat_features=cat_features_50,\n",
    "                            verbose=False,\n",
    "                            thread_count=-1,\n",
    "                            early_stopping_rounds=5\n",
    "                           )\n",
    "                            \n",
    "\n",
    "grid = {'learning_rate': [0.1, 0.13],\n",
    "        'depth': [3,7,9],\n",
    "        'l2_leaf_reg': [9, 13, 17],\n",
    "        'iterations': [1500]\n",
    "       }\n",
    "\n",
    "randomized_search_result = model_3.randomized_search(grid,\n",
    "                                                   X=X_train[selected_features],\n",
    "                                                   y=y_train,\n",
    "                                                   plot=True,\n",
    "                                                   n_iter=5,\n",
    "                                                   cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = randomized_search_result['params']\n",
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'depth': 7, 'l2_leaf_reg': 9, 'iterations': 1500, 'learning_rate': 0.13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CatBoostRegressor(depth=7,\n",
    "                               l2_leaf_reg=9,\n",
    "                               iterations=2000,\n",
    "                               learning_rate=0.13,\n",
    "                               cat_features=cat_features_50,\n",
    "                               thread_count=-1,\n",
    "                               loss_function='Huber:delta=1.1')\n",
    "\n",
    "\n",
    "best_model.fit(X=X_train[selected_features],\n",
    "               y=y_train,\n",
    "               plot=True,\n",
    "               eval_set=(X_validation[selected_features],y_validation),\n",
    "               verbose=False\n",
    "               )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/theoberva/airbnb_steamlit/blob/main/catboost_model.ipynb\n",
    "catboost modeli icin link"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
