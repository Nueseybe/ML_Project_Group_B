{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_13300\\2231637721.py:10: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-whitegrid\")\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import missingno as msno\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from collections import Counter\n",
    "\n",
    "# import library for modeling \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split,  KFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge #ordinary linear regression + w/ ridge regularization\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "\n",
    "#tst-train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(''):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'c:\\\\info_tech_dersler\\\\ml_prj_airbnb\\\\ML_Project_Group_B': ['.git', 'catboost_info', 'Column_description.xlsx', 'data_final.csv', 'Feature_Engineering.ipynb', 'Main.ipynb', 'Missing_Values.ipynb', 'modelling.ipynb', 'Outliers.ipynb', 'README.md', 'train.csv', 'usneighborhoods.csv', 'Visualization.ipynb']\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()  # your current working directory (cwd)\n",
    "files = os.listdir(cwd)  # This will Get all the files in that directory\n",
    "print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_price', 'accommodates', 'bathrooms', 'cleaning_fee',\n",
       "       'host_response_rate', 'instant_bookable', 'number_of_reviews',\n",
       "       'review_scores_rating', 'thumbnail_url', 'bedrooms', 'beds',\n",
       "       'log_price_new', 'property_type_Apartment', 'property_type_bed_break',\n",
       "       'property_type_Bungalow', 'property_type_Condominium',\n",
       "       'property_type_Dorm', 'property_type_Guesthouse', 'property_type_House',\n",
       "       'property_type_Loft', 'property_type_Other', 'property_type_Townhouse',\n",
       "       'cancellation_policy_flexible', 'cancellation_policy_moderate',\n",
       "       'cancellation_policy_strict', 'room_type_entire_home',\n",
       "       'room_type_private_room', 'room_type_shared_room', 'bed_type_Airbed',\n",
       "       'bed_type_Couch', 'bed_type_Futon', 'bed_type_Pull_out_Sofa',\n",
       "       'bed_type_real_Bed', 'city_Boston', 'city_Chicago', 'city_DC',\n",
       "       'city_LA', 'city_NYC', 'city_SF', 'wireless_internet', 'Kitchen',\n",
       "       'Heating', 'Essentials', 'smoke_detector', 'air_conditioning', 'TV',\n",
       "       'Shampoo', 'Hangers', 'carbon_monoxide_detector', 'Internet',\n",
       "       'laptop_friendly_workspace', 'hair_dryer', 'Washer', 'Dryer', 'Iron',\n",
       "       'family_kid_friendly', 'fire_extinguister', 'first_aid_kit', 'cable_tv',\n",
       "       'free_parking_on_premises', 'alltime_check_in', 'lock_on_bedroom_door',\n",
       "       'buzzer_wireless,intercom', 'neighbourhood_level'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('log_price' ,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>thumbnail_url</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>...</th>\n",
       "      <th>Iron</th>\n",
       "      <th>family_kid_friendly</th>\n",
       "      <th>fire_extinguister</th>\n",
       "      <th>first_aid_kit</th>\n",
       "      <th>cable_tv</th>\n",
       "      <th>free_parking_on_premises</th>\n",
       "      <th>alltime_check_in</th>\n",
       "      <th>lock_on_bedroom_door</th>\n",
       "      <th>buzzer_wireless,intercom</th>\n",
       "      <th>neighbourhood_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943841</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943841</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accommodates  bathrooms  cleaning_fee  host_response_rate   \n",
       "0             3        1.0             1            0.943841  \\\n",
       "1             7        1.0             1            1.000000   \n",
       "2             5        1.0             1            1.000000   \n",
       "3             4        1.0             1            0.943841   \n",
       "4             2        1.0             1            1.000000   \n",
       "\n",
       "   instant_bookable  number_of_reviews  review_scores_rating  thumbnail_url   \n",
       "0                 0                  2                 100.0              1  \\\n",
       "1                 1                  6                  93.0              1   \n",
       "2                 1                 10                  92.0              1   \n",
       "3                 0                  0                   0.0              1   \n",
       "4                 1                  4                  40.0              0   \n",
       "\n",
       "   bedrooms  beds  ...  Iron  family_kid_friendly  fire_extinguister   \n",
       "0       1.0   1.0  ...     1                    1                  0  \\\n",
       "1       3.0   3.0  ...     1                    1                  1   \n",
       "2       1.0   3.0  ...     1                    1                  1   \n",
       "3       2.0   2.0  ...     0                    0                  1   \n",
       "4       0.0   1.0  ...     0                    0                  1   \n",
       "\n",
       "   first_aid_kit  cable_tv  free_parking_on_premises  alltime_check_in   \n",
       "0              0         0                         0                 0  \\\n",
       "1              0         0                         0                 0   \n",
       "2              0         1                         0                 0   \n",
       "3              1         1                         0                 0   \n",
       "4              0         0                         0                 0   \n",
       "\n",
       "   lock_on_bedroom_door  buzzer_wireless,intercom  neighbourhood_level  \n",
       "0                     0                         0                    4  \n",
       "1                     0                         0                    1  \n",
       "2                     0                         1                    4  \n",
       "3                     0                         1                    1  \n",
       "4                     0                         0                    4  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', len(df.columns)) # To view all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Feature  Correlation\n",
      "0                   accommodates     0.519225\n",
      "1                      bathrooms     0.255724\n",
      "2                   cleaning_fee     0.136475\n",
      "7                  thumbnail_url    -0.115338\n",
      "8                       bedrooms     0.409696\n",
      "9                           beds     0.414853\n",
      "20  cancellation_policy_flexible    -0.111260\n",
      "22    cancellation_policy_strict     0.137122\n",
      "23         room_type_entire_home     0.620337\n",
      "24        room_type_private_room    -0.548019\n",
      "25         room_type_shared_room    -0.231679\n",
      "30             bed_type_real_Bed     0.108203\n",
      "36                       city_SF     0.166433\n",
      "38                       Kitchen     0.101930\n",
      "39                       Heating     0.105705\n",
      "43                            TV     0.270233\n",
      "44                       Shampoo     0.109371\n",
      "47                      Internet     0.103412\n",
      "49                    hair_dryer     0.131115\n",
      "50                        Washer     0.158085\n",
      "51                         Dryer     0.164773\n",
      "52                          Iron     0.116852\n",
      "53           family_kid_friendly     0.296804\n",
      "56                      cable_tv     0.238058\n",
      "58              alltime_check_in     0.122798\n",
      "59          lock_on_bedroom_door    -0.152593\n",
      "61           neighbourhood_level    -0.149521\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Veri çerçevesindeki korelasyon matrisini oluşturun\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Target değişkeni ile diğer sütunların korelasyonunu alın\n",
    "target_correlation = correlation_matrix['log_price_new'].drop('log_price_new')\n",
    "\n",
    "# Korelasyonları veren tabloyu oluşturun\n",
    "correlation_table = pd.DataFrame({'Feature': target_correlation.index, 'Correlation': target_correlation.values})\n",
    "\n",
    "# -0.3 ile 0 arasındaki ve 0.3 ile 1 arasındaki korelasyonları filtreleyin\n",
    "filtered_correlation = correlation_table[(correlation_table['Correlation'] < -0.1) & (correlation_table['Correlation'] > -1) | (correlation_table['Correlation'] > 0.1) & (correlation_table['Correlation'] < 1)]\n",
    "\n",
    "# Tabloyu ekrana yazdırın\n",
    "print(filtered_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_result = pd.concat([lr_result, dt_result, rf_result, et_result, ad_result, xgb_result, lgbm_result])\n",
    "modeling_result.sort_values(by='NDCG Score', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Model\t                NDCG Score\n",
    "0\tLGBMClassifier\t        0.8496 +/- 0.0006\n",
    "0\tXGBClassifier\t        0.8482 +/- 0.0004\n",
    "0\tRandomForestClassifier\t0.8451 +/- 0.0006\n",
    "0\tAdaBoostClassifier\t    0.8429 +/- 0.0019\n",
    "0\tExtraTreesClassifier\t0.839 +/- 0.0008\n",
    "0\tLogisticRegression\t    0.8378 +/- 0.001\n",
    "0\tDecisionTreeClassifier\t0.7242 +/- 0.0023\n",
    "\n",
    "The <b>Light GBM Classifier</b> model was chosen for hyperparameter tuning, since it's fast to train and tune, whilst being also the one with the best result without any tuning. In addition to that, it's much better for deployment, as it's much lighter than a XGBoost or Random Forest for instance, especially given the fact that we're using a free deployment cloud.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/brunodifranco/project-airbnb-classification/blob/main/airbnb.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important task in ML modeling is Hyperparameter Tuning, on which the goal is to find the <b>best possible combination of model hyperparameters</b>. This task will be performed fitting the model to the training data, and evaluating it in the test data, which was originally split in section 5.2. But firstly the test dataset has to go through all transformations the training dataset went through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_le = LabelEncoder()\n",
    "df_test['country_destination'] = test_le.fit_transform(df_test['country_destination'])\n",
    "\n",
    "X_test = df_test.drop('country_destination', axis=1).copy()\n",
    "y_test = df_test['country_destination'].copy()\n",
    "\n",
    "# One Hot Encoding \n",
    "X_test = pd.get_dummies(X_test, prefix=['gender'], columns=['gender'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['signup_method'], columns=['signup_method'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['signup_app'], columns=['signup_app'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['affiliate_channel'], columns=['affiliate_channel'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['first_affiliate_tracked'], columns=['first_affiliate_tracked'])\n",
    "X_test = pd.get_dummies(X_test, prefix=['first_device_type'], columns=['first_device_type'])\n",
    "\n",
    "# FrequencyEncoder\n",
    "fe_language = (X_test.groupby('language').size()) / len(X_test)\n",
    "X_test['language'] = X_test['language'].apply(lambda x : fe_language[x])\n",
    "\n",
    "fe_action_type_most_common = (X_test.groupby('action_type_most_common').size()) / len(X_test)\n",
    "X_test['action_type_most_common'] = X_test['action_type_most_common'].apply(lambda x : fe_action_type_most_common[x])\n",
    "\n",
    "# RobustScaler\n",
    "rs_age = RobustScaler()\n",
    "rs_signup_flow = RobustScaler()\n",
    "rs_secs_elapsed_median = RobustScaler()\n",
    "X_test['age'] = rs_age.fit_transform(X_test[['age']].values)\n",
    "X_test['signup_flow'] = rs_signup_flow.fit_transform(X_test[['signup_flow']].values)\n",
    "X_test['secs_elapsed_median'] = rs_secs_elapsed_median.fit_transform(X_test[['secs_elapsed_median']].values)\n",
    "\n",
    "# MinMaxScaler\n",
    "mm_secs_elapsed_max = MinMaxScaler()\n",
    "mm_secs_elapsed_mean = MinMaxScaler()\n",
    "mm_secs_elapsed_sum = MinMaxScaler()\n",
    "mm_secs_elapsed_std = MinMaxScaler()\n",
    "mm_amount_of_sessions = MinMaxScaler()\n",
    "X_test['secs_elapsed_max'] = mm_secs_elapsed_max.fit_transform(X_test[['secs_elapsed_max']].values)\n",
    "X_test['secs_elapsed_mean'] = mm_secs_elapsed_mean.fit_transform(X_test[['secs_elapsed_mean']].values)\n",
    "X_test['secs_elapsed_sum'] = mm_secs_elapsed_sum.fit_transform(X_test[['secs_elapsed_sum']].values)\n",
    "X_test['secs_elapsed_std'] = mm_secs_elapsed_std.fit_transform(X_test[['secs_elapsed_std']].values)\n",
    "X_test['amount_of_sessions'] = mm_amount_of_sessions.fit_transform(X_test[['amount_of_sessions']].values)\n",
    "\n",
    "# StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_test['action_type_unique'] = ss.fit_transform(X_test[['action_type_unique']].values)\n",
    "\n",
    "# Transformation\n",
    "cols = {'day_of_week_first_active': 7,   \n",
    "        'month_account_created' : 12, \n",
    "        'day_first_active': 30,\n",
    "        'week_of_year_account_created': 52}\n",
    "\n",
    "for period, cycle in cols.items():\n",
    "    nature_encode(X_test, period, cycle)\n",
    "\n",
    "# final X_test\n",
    "X_test = X_test[cols_selected_rf]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the tested hyperparameters, Bayesian Optimization with Optuna provided the following as the best ones:\n",
    "\n",
    "| Hyperparameter | Definition | Best Value |\n",
    "|:---:|---|:---:|\n",
    "| n_estimators | Number of boosting iterations | 300 |\n",
    "| learning_rate | Shrinkage rate | 0.02 |\n",
    "| num_leaves | max number of leaves in one tree | 45 |\n",
    "| max_depth | Limit the max depth for tree model | 8 |\n",
    "| min_child_samples | Minimal number of data in one leaf | 55 |\n",
    "| min_child_weight | Minimal sum hessian in one leaf | 0.04 |\n",
    "| subsample | Used to randomly select part of data without resampling| 0.8 |\n",
    "| colsample_bytree| Used to randomly select a subset of features on each iteration| 0.85 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned Parameters\n",
    "best_param =  {'n_estimators': 300,\n",
    "               'learning_rate': 0.02,\n",
    "               'num_leaves': 45,\n",
    "               'max_depth': 8,\n",
    "               'min_child_samples': 55,\n",
    "               'min_child_weight': 0.04,\n",
    "               'subsample': 0.8,\n",
    "               'colsample_bytree': 0.85}\n",
    "\n",
    "# Final Model\n",
    "model_lgbm_final = LGBMClassifier(**best_param)\n",
    "\n",
    "y_pred_eval, df_model_eval = model_eval(model_lgbm_final, X_train_ml, y_train_ml, X_test, y_test)\n",
    "# pickle.dump(model_lgbm_final, open('model/lgbm_airbnb.pkl', 'wb'))  # Saving for deployment\n",
    "\n",
    "df_model_eval # final model score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgbm i secmis ve hyper parametreleri bulup tune etmis \n",
    "\n",
    "https://github.com/brunodifranco/project-airbnb-classification/blob/main/airbnb.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features must be str or int for catboost model\n",
    "\n",
    "df[cat_features] = df[cat_features].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(iterations=2000, \n",
    "                          learning_rate=0.05,\n",
    "                          depth=10,\n",
    "                          l2_leaf_reg=15,\n",
    "                          loss_function='Huber:delta=1.6',\n",
    "                          # save_snapshot='\n",
    "                         )\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=cat_features,\n",
    "    verbose=250,\n",
    "    eval_set=(X_validation, y_validation),\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "Look at feature importance and chose only the features that help the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portances = model.get_feature_importance()\n",
    "\n",
    "sorted_importances = sorted(importances, reverse=True)\n",
    "\n",
    "threshold = sorted_importances[len(sorted_importances) // 2]\n",
    "\n",
    "selected_features = [f for i, f in enumerate(X_train.columns) if importances[i] >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new list of categorical features in the new df\n",
    "all_columns = list(X_train[selected_features].columns)\n",
    "num_columns = ['accommodates', 'bedrooms', 'beds', 'minimum_nights', 'maximum_nights', 'bathrooms', 'price']\n",
    "\n",
    "cat_features_50 = []\n",
    "for column in all_columns:\n",
    "    if column not in num_columns:\n",
    "        cat_features_50.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with top 50 features\n",
    "model_2 = CatBoostRegressor(iterations=2000, \n",
    "                          learning_rate=0.08,\n",
    "                          depth=10,\n",
    "                          l2_leaf_reg=15,\n",
    "                          loss_function='Huber:delta=1.1',\n",
    "                          #save_snapshot=True\n",
    "                         )\n",
    "\n",
    "model_2.fit(\n",
    "    X_train[selected_features], y_train,\n",
    "    cat_features=cat_features_50,\n",
    "    verbose=250,\n",
    "    eval_set=(X_validation[selected_features], y_validation),\n",
    "    plot=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning\n",
    "model_3 = CatBoostRegressor(loss_function='Huber:delta=1.1',\n",
    "                            cat_features=cat_features_50,\n",
    "                            verbose=False,\n",
    "                            thread_count=-1,\n",
    "                            early_stopping_rounds=5\n",
    "                           )\n",
    "                            \n",
    "\n",
    "grid = {'learning_rate': [0.1, 0.13],\n",
    "        'depth': [3,7,9],\n",
    "        'l2_leaf_reg': [9, 13, 17],\n",
    "        'iterations': [1500]\n",
    "       }\n",
    "\n",
    "randomized_search_result = model_3.randomized_search(grid,\n",
    "                                                   X=X_train[selected_features],\n",
    "                                                   y=y_train,\n",
    "                                                   plot=True,\n",
    "                                                   n_iter=5,\n",
    "                                                   cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = randomized_search_result['params']\n",
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'depth': 7, 'l2_leaf_reg': 9, 'iterations': 1500, 'learning_rate': 0.13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CatBoostRegressor(depth=7,\n",
    "                               l2_leaf_reg=9,\n",
    "                               iterations=2000,\n",
    "                               learning_rate=0.13,\n",
    "                               cat_features=cat_features_50,\n",
    "                               thread_count=-1,\n",
    "                               loss_function='Huber:delta=1.1')\n",
    "\n",
    "\n",
    "best_model.fit(X=X_train[selected_features],\n",
    "               y=y_train,\n",
    "               plot=True,\n",
    "               eval_set=(X_validation[selected_features],y_validation),\n",
    "               verbose=False\n",
    "               )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/theoberva/airbnb_steamlit/blob/main/catboost_model.ipynb\n",
    "catboost modeli icin link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
